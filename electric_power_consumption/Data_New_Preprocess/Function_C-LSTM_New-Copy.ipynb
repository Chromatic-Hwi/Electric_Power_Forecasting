{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49534bc4",
   "metadata": {},
   "source": [
    "# CNN+LSTM 전력 수요 예측 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4a87c8",
   "metadata": {},
   "source": [
    "### <목차>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f886a9",
   "metadata": {},
   "source": [
    "### <화면 가로 확장>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01779336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 화면 가로 확장 코드 (기본 width 50%)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914b761",
   "metadata": {},
   "source": [
    "### 1. 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fef1b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, LSTM, Bidirectional, MaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137a3ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimpleGraph(title, plot, ylabel, savename):\n",
    "    plt.figure(figsize=(150,20))\n",
    "    plt.grid()\n",
    "    plt.title(title, fontsize=180)\n",
    "    plt.plot(plot)\n",
    "\n",
    "    plt.xlabel(\"Time by Hour\", fontsize=130)\n",
    "    plt.ylabel(ylabel, fontsize=130)\n",
    "    plt.margins(x=0.002)\n",
    "\n",
    "    plt.xticks(list_24, labels=np.arange(24))\n",
    "\n",
    "    plt.tick_params(axis='x', size=15)\n",
    "    plt.tick_params(axis='x', labelsize=70)\n",
    "    plt.tick_params(axis='y', size=15)\n",
    "    plt.tick_params(axis='y', labelsize=70)\n",
    "    plt.savefig(\"./Result/\"+savename+\".png\", bbox_inches='tight',pad_inches=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d96445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2102400 entries, 0 to 2102399\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Dtype  \n",
      "---  ------           -----  \n",
      " 0   Unnamed: 0       int64  \n",
      " 1   Year-Month-Date  object \n",
      " 2   Time             object \n",
      " 3   Holiday          int64  \n",
      " 4   Watt             float64\n",
      " 5   Temp('C)         float64\n",
      " 6   Humidity(%)      float64\n",
      " 7   CPI              float64\n",
      "dtypes: float64(4), int64(2), object(2)\n",
      "memory usage: 128.3+ MB\n"
     ]
    }
   ],
   "source": [
    "Data_4Y_Edited = pd.read_csv('./Combined_Data_New/House1_Ch1_Combined_Data_New_13-16_Outlier_Edited.csv', encoding='cp949')\n",
    "Data_4Y_Edited.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9811185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1000)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[-1]))\n",
    "    return ds.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9b1db7",
   "metadata": {},
   "source": [
    "### 편하게 학습하기 위한 함수(추후 본문 합류 및 삭제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3625badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Auto_Scaler_lr_filt_kernel_merge_case(scaler_name, lr, filt, kernel, mode, case):\n",
    "    start=datetime.datetime.now()\n",
    "    print(start, \"\\n\")\n",
    "    Data_4Y_Edited.sort_index(ascending=False).reset_index(drop=True)\n",
    "    from sklearn.preprocessing import PowerTransformer, QuantileTransformer, StandardScaler\n",
    "    scaler=scaler_name\n",
    "\n",
    "    scale_cols = [\"Holiday\", \"Watt\", \"Temp('C)\", \"Humidity(%)\", \"CPI\"]\n",
    "    Data_4Y_scaled = scaler.fit_transform(Data_4Y_Edited[scale_cols])\n",
    "    Data_4Y_scaled = pd.DataFrame(Data_4Y_scaled)\n",
    "    Data_4Y_scaled.columns = scale_cols\n",
    "    Data_4Y_scaled.insert(2, \"Watt2\", Data_4Y_scaled[\"Watt\"])\n",
    "    if case==1:\n",
    "        Data_4Y_scaled=Data_4Y_scaled[[\"Watt\", \"Watt2\"]]\n",
    "    if case==2:\n",
    "        Data_4Y_scaled=Data_4Y_scaled[[\"Watt\", \"Temp('C)\", \"Watt2\"]]\n",
    "    if case==3:\n",
    "        Data_4Y_scaled=Data_4Y_scaled[[\"Watt\", \"Humidity(%)\", \"Watt2\"]]\n",
    "    if case==4:\n",
    "        Data_4Y_scaled=Data_4Y_scaled[[\"Watt\", \"Humidity(%)\", \"Watt2\"]]\n",
    "    if case==5:\n",
    "        Data_4Y_scaled=Data_4Y_scaled[[\"Watt\", \"Temp('C)\", \"Humidity(%)\", \"Watt2\"]]\n",
    "    if case==6:\n",
    "        Data_4Y_scaled=Data_4Y_scaled[[\"Watt\", \"Temp('C)\", \"Humidity(%)\", \"Holiday\", \"Watt2\"]]\n",
    "    Data_4Y_scaled.info()\n",
    "    \n",
    "    #=================================================================================\n",
    "    TEST_SIZE = 1*60*24*365 #1년치를 테스트셋으로 사용\n",
    "    WINDOW_SIZE = 1\n",
    "    BATCH_SIZE = 32\n",
    "    VAL_DAYS = 525600\n",
    "\n",
    "    learning = Data_4Y_scaled[:-TEST_SIZE] # 학습에 쓰이는 전체 데이터\n",
    "    VAL_RATIO=VAL_DAYS/len(learning)\n",
    "    VAL_SIZE = int(len(learning)*VAL_RATIO)\n",
    "    TRAIN = learning[:-VAL_SIZE]\n",
    "    VAL = learning[-VAL_SIZE:]\n",
    "    TEST = Data_4Y_scaled[-TEST_SIZE:]\n",
    "\n",
    "    train_data = windowed_dataset(TRAIN, WINDOW_SIZE, BATCH_SIZE, False) \n",
    "    valid_data = windowed_dataset(VAL, WINDOW_SIZE, BATCH_SIZE, True)\n",
    "    test_data = windowed_dataset(TEST, WINDOW_SIZE, BATCH_SIZE, False)\n",
    "\n",
    "    #===========================================================================\n",
    "    LeakyReLU=tf.keras.layers.LeakyReLU(alpha=0.1)\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=filt, kernel_size=kernel, padding=\"causal\", activation=LeakyReLU, input_shape=[WINDOW_SIZE, TRAIN.shape[1]]),\n",
    "        MaxPooling1D(pool_size=2, strides=1, padding=\"same\"),\n",
    "        Conv1D(filters=filt, kernel_size=kernel, padding=\"causal\", activation=LeakyReLU),\n",
    "        MaxPooling1D(pool_size=2, strides=1, padding=\"same\"),\n",
    "        Bidirectional(LSTM(4, activation=LeakyReLU, return_sequences=True), merge_mode=mode),\n",
    "        TimeDistributed(Dense(8)),\n",
    "        Bidirectional(LSTM(4, activation=LeakyReLU, return_sequences=True), merge_mode=mode),\n",
    "        TimeDistributed(Dense(4)),\n",
    "        Bidirectional(LSTM(2, activation=LeakyReLU, return_sequences=False), merge_mode=mode),\n",
    "        Dense(1)\n",
    "                        ])\n",
    "    Nadam = tf.keras.optimizers.Nadam(learning_rate=lr)\n",
    "    model.compile(loss='mean_absolute_error', optimizer=Nadam)\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "    with tf.device('/GPU:0'):\n",
    "        model_path = 'model'\n",
    "        filename = os.path.join(model_path, \"tmp_checkpoint_CL_\"+str(scaler_name)+\"_\"+str(lr)+\"_\"+str(filt)+\"_\"+str(kernel)+\"_\"+mode+\"_\"+str(case)+\".h5\")\n",
    "        checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "        history = model.fit(train_data, epochs=30, batch_size=BATCH_SIZE, validation_data=(valid_data), callbacks=[checkpoint, early_stop])\n",
    "\n",
    "    model.load_weights(filename)\n",
    "    pred = model.predict(test_data)\n",
    "    pred.shape\n",
    "    #================================================================================================================\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.title('Model Loss Convergence Graph', size='15')\n",
    "    y_tloss = history.history['loss']\n",
    "    y_vloss = history.history['val_loss']\n",
    "    x_len = np.arange(len(y_tloss))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    #plt.xticks(range(0,50,10), labels=range(1,21, 10))\n",
    "\n",
    "    plt.plot(x_len, y_tloss, \"o-\", c=\"blue\", markersize=3)\n",
    "    plt.plot(x_len, y_vloss, \"o-\", c=\"red\", markersize=3)\n",
    "    plt.margins(x=0.02)\n",
    "    plt.legend(['Training loss', 'Validation loss'])\n",
    "    plt.savefig(\"./Result/CNN+LSTM/Graph/Model_Loss_Convergence_Graph_\"+str(scaler_name)+\"_\"+str(lr)+\"_\"+str(filt)+\"_\"+str(kernel)+\"_\"+mode+\"_\"+str(case)+\".png\")\n",
    "\n",
    "    #=================================================================================================================\n",
    "    fig = plt.figure(figsize=(300,15)) \n",
    "    ax1 = fig.add_subplot() # subplot 그래프 생성\n",
    "    ax1.tick_params(axis='y', size=20, labelsize=20) # y축 눈금 표기 설정\n",
    "    plt.yticks([0, 0.111, 0.222, 0.333, 0.444, 0.555, 0.666, 0.777, 0.888, 1.0], \n",
    "               labels=['0', '1000 W', '2000 W', '3000 W', '4000 W', '5000 W', '6000 W', '7000 W', '8000 W','9000 W'])\n",
    "\n",
    "    color1 = 'darkorange'\n",
    "    ax1.plot(TEST['Watt'][:-WINDOW_SIZE], color=color1)\n",
    "    ax1.tick_params(axis='y')\n",
    "    ax1.tick_params(axis='x', size=20, labelsize=20)\n",
    "    ax1.margins(x=0.005)\n",
    "\n",
    "    ax1.legend(['Actual'], loc=1, bbox_to_anchor=(0.995, 1, 0, 0))\n",
    "\n",
    "    color2 = 'blue'\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(pred, color=color2)\n",
    "    ax2.tick_params(axis='x', size=20, labelsize=20)\n",
    "    ax2.margins(x=0.005)\n",
    "\n",
    "    ax2.legend(['Predict'], loc=1, bbox_to_anchor=(1, 1, 0, 0))\n",
    "\n",
    "    plt.savefig(\"./Result/CNN+LSTM/Graph/Pred_with_Actual_\"+str(scaler_name)+\"_\"+str(lr)+\"_\"+str(filt)+\"_\"+str(kernel)+\"_\"+mode+\"_\"+str(case)+\".png\")\n",
    "    \n",
    "    from sklearn.metrics import r2_score as r2\n",
    "    from sklearn.metrics import mean_absolute_error as MAE\n",
    "    from sklearn.metrics import mean_squared_error as MSE\n",
    "    def SMAPE(y_test, y_pred):\n",
    "        return np.mean((np.abs(y_test-y_pred))/(np.abs(y_test)+np.abs(y_pred)))*100/2\n",
    "    \n",
    "    true = TEST['Watt'][:-WINDOW_SIZE].to_numpy()\n",
    "    true = true.reshape(-1,1)\n",
    "\n",
    "    MAE = MAE(true, pred)\n",
    "    RMSE = np.sqrt(MSE(true, pred))\n",
    "    SMAPE0 = SMAPE(true, pred)\n",
    "    \n",
    "    print('\\nr2 >> %.4f' %r2(true, pred)) # 1에 가까워야 좋음\n",
    "    print('MAE >> %.4f' %MAE) # 0에 가까워야 좋음\n",
    "    print('RMSE >> %.4f' %RMSE) # 0에 가까워야 좋음\n",
    "    print('SMAPE >> %.4f' %SMAPE0) # 0에 가까워야 좋음\n",
    "    end=datetime.datetime.now()\n",
    "    print(\"\\n소요시간 >>\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3269ee0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-29 11:06:36.335311 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2102400 entries, 0 to 2102399\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   Watt         float64\n",
      " 1   Temp('C)     float64\n",
      " 2   Humidity(%)  float64\n",
      " 3   Watt2        float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 64.2 MB\n",
      "WARNING:tensorflow:Layer lstm_68 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_68 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_68 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_70 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_70 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_70 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/30\n",
      "  32850/Unknown - 1486s 45ms/step - loss: 0.2585\n",
      "Epoch 00001: val_loss improved from inf to 0.25896, saving model to model\\tmp_checkpoint_CL_MinMaxScaler()_0.0004_16_4_sum_5.h5\n",
      "32850/32850 [==============================] - 1573s 48ms/step - loss: 0.2585 - val_loss: 0.2590\n",
      "Epoch 2/30\n",
      "32850/32850 [==============================] - ETA: 0s - loss: 0.2581\n",
      "Epoch 00002: val_loss improved from 0.25896 to 0.25480, saving model to model\\tmp_checkpoint_CL_MinMaxScaler()_0.0004_16_4_sum_5.h5\n",
      "32850/32850 [==============================] - 1608s 49ms/step - loss: 0.2581 - val_loss: 0.2548\n",
      "Epoch 3/30\n",
      "32850/32850 [==============================] - ETA: 0s - loss: 0.2581\n",
      "Epoch 00003: val_loss improved from 0.25480 to 0.25399, saving model to model\\tmp_checkpoint_CL_MinMaxScaler()_0.0004_16_4_sum_5.h5\n",
      "32850/32850 [==============================] - 1658s 50ms/step - loss: 0.2581 - val_loss: 0.2540\n",
      "Epoch 4/30\n",
      "32849/32850 [============================>.] - ETA: 0s - loss: 0.2580\n",
      "Epoch 00004: val_loss improved from 0.25399 to 0.25357, saving model to model\\tmp_checkpoint_CL_MinMaxScaler()_0.0004_16_4_sum_5.h5\n",
      "32850/32850 [==============================] - 1564s 48ms/step - loss: 0.2580 - val_loss: 0.2536\n",
      "Epoch 5/30\n",
      "32850/32850 [==============================] - ETA: 0s - loss: 0.2579\n",
      "Epoch 00005: val_loss improved from 0.25357 to 0.25257, saving model to model\\tmp_checkpoint_CL_MinMaxScaler()_0.0004_16_4_sum_5.h5\n",
      "32850/32850 [==============================] - 1527s 46ms/step - loss: 0.2579 - val_loss: 0.2526\n",
      "Epoch 6/30\n",
      "32849/32850 [============================>.] - ETA: 0s - loss: 0.2577\n",
      "Epoch 00006: val_loss improved from 0.25257 to 0.25176, saving model to model\\tmp_checkpoint_CL_MinMaxScaler()_0.0004_16_4_sum_5.h5\n",
      "32850/32850 [==============================] - 1532s 47ms/step - loss: 0.2577 - val_loss: 0.2518\n",
      "Epoch 7/30\n",
      "32849/32850 [============================>.] - ETA: 0s - loss: 0.2576\n",
      "Epoch 00007: val_loss did not improve from 0.25176\n",
      "32850/32850 [==============================] - 1524s 46ms/step - loss: 0.2576 - val_loss: 0.2518\n",
      "Epoch 8/30\n",
      "32849/32850 [============================>.] - ETA: 0s - loss: 0.2576\n",
      "Epoch 00008: val_loss improved from 0.25176 to 0.25176, saving model to model\\tmp_checkpoint_CL_MinMaxScaler()_0.0004_16_4_sum_5.h5\n",
      "32850/32850 [==============================] - 1550s 47ms/step - loss: 0.2576 - val_loss: 0.2518\n",
      "Epoch 9/30\n",
      "32849/32850 [============================>.] - ETA: 0s - loss: 0.2576\n",
      "Epoch 00009: val_loss improved from 0.25176 to 0.25173, saving model to model\\tmp_checkpoint_CL_MinMaxScaler()_0.0004_16_4_sum_5.h5\n",
      "32850/32850 [==============================] - 1564s 48ms/step - loss: 0.2576 - val_loss: 0.2517\n",
      "Epoch 10/30\n",
      "32849/32850 [============================>.] - ETA: 0s - loss: 0.2576\n",
      "Epoch 00010: val_loss did not improve from 0.25173\n",
      "32850/32850 [==============================] - 1548s 47ms/step - loss: 0.2576 - val_loss: 0.2518\n",
      "Epoch 11/30\n",
      "32849/32850 [============================>.] - ETA: 0s - loss: 0.2576\n",
      "Epoch 00011: val_loss did not improve from 0.25173\n",
      "32850/32850 [==============================] - 1538s 47ms/step - loss: 0.2576 - val_loss: 0.2518\n",
      "Epoch 12/30\n",
      "32849/32850 [============================>.] - ETA: 0s - loss: 0.2576\n",
      "Epoch 00012: val_loss did not improve from 0.25173\n",
      "32850/32850 [==============================] - 1540s 47ms/step - loss: 0.2576 - val_loss: 0.2519\n",
      "Epoch 13/30\n",
      "22289/32850 [===================>..........] - ETA: 7:54 - loss: 0.2499"
     ]
    }
   ],
   "source": [
    "Auto_Scaler_lr_filt_kernel_merge_case(MinMaxScaler(), 0.0004, 16, 4, \"sum\", 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
